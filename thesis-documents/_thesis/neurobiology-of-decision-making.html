<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>3 Neurobiology of Decision-Making | Counterfactuals, Dopamine, and Risky Behavior</title>
<meta name="author" content="Jonathan D. Trattner">
<meta name="description" content="In the last chapter, I used the term prospect to describe a problem that has two options with stated probabilities and outcomes. In the experimental literature, these prospects are...">
<meta name="generator" content="bookdown 0.25 with bs4_book()">
<meta property="og:title" content="3 Neurobiology of Decision-Making | Counterfactuals, Dopamine, and Risky Behavior">
<meta property="og:type" content="book">
<meta property="og:url" content="https://masters-thesis.jdtrat.com/neurobiology-of-decision-making.html">
<meta property="og:description" content="In the last chapter, I used the term prospect to describe a problem that has two options with stated probabilities and outcomes. In the experimental literature, these prospects are...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3 Neurobiology of Decision-Making | Counterfactuals, Dopamine, and Risky Behavior">
<meta name="twitter:site" content="@jdtrat">
<meta name="twitter:description" content="In the last chapter, I used the term prospect to describe a problem that has two options with stated probabilities and outcomes. In the experimental literature, these prospects are...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Deriving a neurobiological theory of decision-making under risk">Counterfactuals, Dopamine, and Risky Behavior</a>:
        <small class="text-muted">Deriving a neurobiological theory of decision-making under risk</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Hello, World</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="evolution-of-decision-theory.html"><span class="header-section-number">2</span> Evolution of Decision Theory</a></li>
<li><a class="active" href="neurobiology-of-decision-making.html"><span class="header-section-number">3</span> Neurobiology of Decision-Making</a></li>
<li><a class="" href="counterfactual-predicted-utility-theory.html"><span class="header-section-number">4</span> Counterfactual Predicted Utility Theory</a></li>
<li><a class="" href="computational-modeling-concepts.html"><span class="header-section-number">5</span> Computational Modeling Concepts</a></li>
<li><a class="" href="modeling-methods-and-results.html"><span class="header-section-number">6</span> Modeling Methods and Results</a></li>
<li><a class="" href="references.html"><span class="header-section-number">7</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/jdtrat/masters-thesis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="neurobiology-of-decision-making" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Neurobiology of Decision-Making<a class="anchor" aria-label="anchor" href="#neurobiology-of-decision-making"><i class="fas fa-link"></i></a>
</h1>
<p>In the last chapter, I used the term prospect to describe a problem that has two options with stated probabilities and outcomes. In the experimental literature, these prospects are ‘description-based.’ Formally, researchers who use description-based prospects in their experiments ask participants to select one of the available options that are presented along with a complete description of a non-trivial problem.<span class="citation">(<a href="references.html#ref-barron2003a" role="doc-biblioref">Barron and Erev 2003</a>)</span></p>
<p>When looking towards the neuroscientific literature of decision-making, a different type of experimental paradigm is perhaps more common: ‘experiential’ or ‘feedback-based’ decisions.<span class="citation">(<a href="references.html#ref-barron2003a" role="doc-biblioref">Barron and Erev 2003</a>)</span>. By interacting with the environment, humans (and other animals) associate the consequences of different actions and adapt their behavior accordingly. In other words, we learn. We learn over time that we want to eat sushi for dinner, it’s faster to take the highway to work, and we’re better off if we go to sleep before midnight.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Although I simulated participants’ counterfactual weighting term following the posterior from Kahneman and Tversky’s choice proportion data, &lt;span class="math inline"&gt;\(\gamma \sim \text{Beta}(1.1,1.1)\)&lt;/span&gt;, I used a uniform prior again when sampling from the Metropolis-Hastings algorithm for optimizing my analytical workflow. I felt this was justified because &lt;span class="math inline"&gt;\(\text{Beta}(1.1,1.1)\)&lt;/span&gt; is (relatively) uninformative prior and the decision would not have a large impact on the recoverability of parameter values (with the possible exception of model fitting time). Further, I wanted to maintain consistency with the uniform distribution of the individual-level priors defined in the hierarchical Bayesian model with the inverse Probit transformation.&lt;span class="citation"&gt;(&lt;a href="references.html#ref-ahn2014" role="doc-biblioref"&gt;Ahn et al. 2014&lt;/a&gt;; &lt;a href="references.html#ref-ahn2017" role="doc-biblioref"&gt;Ahn, Haines, and Zhang 2017&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;'><sup>3</sup></a></p>
<p>Looking towards the animal learning field, we see many accounts of experiential learning. To list a few:</p>
<ul>
<li><p>Thorndike’s Law of Effect states that an animal’s behavior can be modified by the consequences of an action.<span class="citation">(<a href="references.html#ref-thorndike1911" role="doc-biblioref">Thorndike 1911</a>)</span></p></li>
<li><p>Classical (Pavlovian) conditioning details learning to predict rewards or punishments from a stimuli independent of any action-taken.<span class="citation">(<a href="references.html#ref-pavlov2010" role="doc-biblioref">Pavlov 2010</a>)</span></p></li>
<li><p>Operant (instrumental) conditioning involves learning how rewards and punishments are contingent upon one’s actions.<span class="citation">(<a href="references.html#ref-b.f.skinner1938" role="doc-biblioref">B. F. Skinner 1938</a>)</span></p></li>
</ul>
<p>That a behavior (an action or decision) is predicated upon prior experiences allows us to formalize the relationship between states. Here, I use state in reference to the representation of stimuli. For example, the state <span class="math inline">\(s \in S\)</span> at time <span class="math inline">\(t\)</span> (<span class="math inline">\(s_t\)</span>) may be comprised of internal states (hunger, thirst, fatigue) or external ones (light, music, temperature).</p>
<p>Consider Pavlov’s experiment which showed how repeatedly giving a dog food after ringing a bell will condition the dog to salivate after the bell is rung.<span class="citation">(<a href="references.html#ref-pavlov2010" role="doc-biblioref">Pavlov 2010</a>)</span> This means that, over time, behavior elicited by a stimulus (food) can be evoked by a – previously – neutral stimulus (the bell). In one of the first attempts to empirically describe this process, Bush and Mosteller suggest the probability of Pavlov’s dog salivating, <span class="math inline">\(P(\text{sal})\)</span>, on the <em>next</em> presentation of food with the bell (next trial, <span class="math inline">\(tr+1\)</span>) is a function of what happened <em>last</em> presentation (last trial, <span class="math inline">\(tr-1\)</span>) discounted by what was experienced during <em>this</em> presentation (food reward this trial, <span class="math inline">\(R_{tr}\)</span>.<span class="citation">(<a href="references.html#ref-bush1951" role="doc-biblioref">Bush and Mosteller 1951a</a>, <a href="references.html#ref-bush1951a" role="doc-biblioref">1951b</a>)</span></p>
<span class="math display" id="eq:bush-mosteller">\[\begin{equation}
P(\text{sal})_{\text{tr}+1} = P(\text{sal})_{\text{tr}-1} + \alpha (R_{\text{tr}} - P(\text{sal})_{\text{tr}-1})
\tag{3.1}
\end{equation}\]</span>
<p>Equation <a href="neurobiology-of-decision-making.html#eq:bush-mosteller">(3.1)</a> computes an average of previously experienced rewards. <span class="math inline">\(0 \leq \alpha \leq 1\)</span> is a learning rate, which modulates the influence of more recent rewards. Bush and Mosteller’s work forms the basis for modern approaches to this problem of <strong><em>reinforcement learning</em></strong>.<span class="citation">(<a href="references.html#ref-glimcher2011a" role="doc-biblioref">Glimcher 2011</a>)</span><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Note that this is a form of Bayes’ theorem that excludes the marginal likelihood of the data – that is, the total probability of observing the data – which acts as a normalizing constant to ensure the posterior distribution is a valid probability density function. For more details on this ‘posterior shortcut,’ see section 2.3.6 of &lt;em&gt;Bayes Rules!&lt;/em&gt;.&lt;span class="citation"&gt;(&lt;a href="references.html#ref-dogucu" role="doc-biblioref"&gt;Dogucu 2022&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;'><sup>4</sup></a> While discretizing learning into trials is a logical first step to modeling behavior in experimental settings, it’s not easily applied to biological systems that continuously interact with their environments.</p>
<p>In his 1988 paper, <em>Learning to Predict by the Methods of Temporal Differences</em>, Richard Sutton introduces the antecedent for temporal-difference reinforcement learning (TDRL) algorithms.<span class="citation">(<a href="references.html#ref-sutton1988" role="doc-biblioref">Sutton 1988</a>)</span> The TDRL algorithm provides a computational framework for optimally learning from experience how actions and their associated stimuli lead to rewards. <span class="citation">(<a href="references.html#ref-sutton-barto-2018" role="doc-biblioref">Richard S. Sutton; Andrew G. Barto 2018</a>)</span></p>
<p>The ‘goal’ of TDRL algorithms are to estimate the value of a state and use that information to maximize rewards. This is achieved with a ‘teaching signal,’ called the temporal-difference reward prediction error (TD-RPE), which relates the value of being in a state at time <span class="math inline">\(t\)</span> to what was expected previously.</p>
<span class="math display" id="eq:td-rpe">\[\begin{equation}
\delta_t = [\text{outcome}_t + \gamma V(S_{t+1})] - V(S_t)
\tag{3.2}
\end{equation}\]</span>
<p>The current value of a state is the sum of any outcome experienced at time <span class="math inline">\(t\)</span> plus the expectation of future outcomes from being in said state. This expectation of future values, <span class="math inline">\(V(S_{t+1})\)</span> is modulated by the temporal-discounting parameter <span class="math inline">\(0 \leq \gamma \leq 1\)</span> and can preferentially weight more immediate outcomes relative to future ones.</p>
<p>In total, Equation <a href="neurobiology-of-decision-making.html#eq:td-rpe">(3.2)</a> shows the TD-RPE, <span class="math inline">\(\delta_t\)</span>, as the difference between the current value of a state and the most recent expectation for that state’s value, <span class="math inline">\(V(S_t)\)</span>. On each time step, <span class="math inline">\(\delta_t\)</span> is used to update the estimated value of the current state as formulated in <a href="neurobiology-of-decision-making.html#eq:td-value-update">(3.3)</a>. Here, <span class="math inline">\(0 \leq \alpha \leq 1\)</span> is the learning rate and controls how much weight an individual places on the estimated value of the current state in light of the TD-RPE.</p>
<span class="math display" id="eq:td-value-update">\[\begin{equation}
\hat{V}(S_t) \leftarrow V(S_t) + (\alpha \cdot \delta_t)
\tag{3.3}
\end{equation}\]</span>
<p>In the 1990s, Read Montague, Peter Dayan, and colleagues show evidence suggesting that TD-RPEs are reflected by fluctuations in the activity of mesencephalic dopaminergic neurons.<span class="citation">(<a href="references.html#ref-montague1996" role="doc-biblioref">P. Montague, Dayan, and Sejnowski 1996</a>; <a href="references.html#ref-schultz1997" role="doc-biblioref">W. Schultz, Dayan, and Montague 1997a</a>)</span> The TD-RPE hypothesis of dopamine neurons is consistent with behavioral and neural results in rodents, <span class="citation">(<a href="references.html#ref-hart2014" role="doc-biblioref">Hart et al. 2014</a>)</span> non-human primates, <span class="citation">(<a href="references.html#ref-tomasljunberg1992" role="doc-biblioref">Tomas Ljunberg, Paul Apicella, and Wolfram Schultz 1992</a>; <a href="references.html#ref-schultz1993" role="doc-biblioref">W. Schultz, Apicella, and Ljungberg 1993</a>; <a href="references.html#ref-schultz1998" role="doc-biblioref">W. Schultz, Tremblay, and Hollerman 1998</a>; <a href="references.html#ref-bayer2005" role="doc-biblioref">Bayer and Glimcher 2005a</a>)</span> and humans.<span class="citation">(<a href="references.html#ref-zaghloul2009" role="doc-biblioref">Zaghloul et al. 2009</a>; <a href="references.html#ref-kishida2011" role="doc-biblioref">Kishida et al. 2011</a>; <a href="references.html#ref-moran2018" role="doc-biblioref">Moran et al. 2018</a>; <a href="references.html#ref-bang2020" role="doc-biblioref">Bang et al. 2020</a>)</span> Because of this biologically conserved mechanism for experiential learning, we can investigate the neural basis of choice behavior empirically with TDRL algorithms. In 2011, these experiments saw an exciting development when Ken Kishida and colleagues used fast-scan cyclic voltammetry to measure dopamine fluctuations in the human striatum with a sub-second temporal resolution. <span class="citation">(<a href="references.html#ref-kishida2011" role="doc-biblioref">Kishida et al. 2011</a>)</span></p>
<p>In combination with their 2016 paper, Kishida and colleagues studied the dopamine levels of humans undergoing elective surgery as part of deep-brain stimulation treatment.<span class="citation">(<a href="references.html#ref-kishida2011" role="doc-biblioref">Kishida et al. 2011</a>, <a href="references.html#ref-kishida2016" role="doc-biblioref">2016</a>)</span> Participants viewed a graphical depiction of a (fictitous) stock market’s price history (Figure <a href="neurobiology-of-decision-making.html#fig:kishida2016-task-design">3.1</a>; B) and were asked to choose how much of their portfolio (initially valued at $100) they would invest. For six ‘markets,’ each with twenty-decisions, participants used handheld button boxes (Figure <a href="neurobiology-of-decision-making.html#fig:kishida2016-task-design">3.1</a>; A) to increase or decrease their investment in ten-percent increments. Decision were made while dopamine was recorded from the striatum in light of three pieces of information:</p>
<ol style="list-style-type: decimal">
<li>The history of the market price (Figure <a href="neurobiology-of-decision-making.html#fig:kishida2016-task-design">3.1</a>; red-trace in panel B)</li>
<li>The current portfolio value (Figure <a href="neurobiology-of-decision-making.html#fig:kishida2016-task-design">3.1</a>; bottom left box, 126, panel B)</li>
<li>The most recent fractional change in portfolio value (Figure <a href="neurobiology-of-decision-making.html#fig:kishida2016-task-design">3.1</a>; bottom right box, 15.2%, panel B).</li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kishida2016-task-design"></span>
<img src="figures/kishida2016-task-design.png" alt="(A) Participants played a sequential-choice game during surgery using button boxes (Left) and a visual display (Right). For each patient, bet size adjustments (e.g., increase bet or decrease bet) and the decision to submit one’s answer were performed with button boxes. (B) Investment game (19, 21): participants view a graphical depiction of the market price history (red trace), their current portfolio value (bottom left box), and their most recent outcome (bottom right box) to decide and submit investment decisions (bets) using a slider bar in 10% increments (bottom center). Bet sizes were limited to 0–100% (in 10% increments) of the participant’s portfolio—no shorting of the market was allowed. During an experiment, a participant played 6 markets with 20 decisions made per market. (C) Timeline of events during a single round of the investment game.*Reprinted with permission from Kishida et al., 2016.*" width="90%"><p class="caption">
Figure 3.1: (A) Participants played a sequential-choice game during surgery using button boxes (Left) and a visual display (Right). For each patient, bet size adjustments (e.g., increase bet or decrease bet) and the decision to submit one’s answer were performed with button boxes. (B) Investment game (19, 21): participants view a graphical depiction of the market price history (red trace), their current portfolio value (bottom left box), and their most recent outcome (bottom right box) to decide and submit investment decisions (bets) using a slider bar in 10% increments (bottom center). Bet sizes were limited to 0–100% (in 10% increments) of the participant’s portfolio—no shorting of the market was allowed. During an experiment, a participant played 6 markets with 20 decisions made per market. (C) Timeline of events during a single round of the investment game.<em>Reprinted with permission from Kishida et al., 2016.</em>
</p>
</div>
<p>In their 2011 paper, they asked one person, MH, to complete the sequential investment task. In line with prior work relating dopamine with unexpected financial outcomes <span class="citation">(<a href="references.html#ref-zaghloul2009" role="doc-biblioref">Zaghloul et al. 2009</a>)</span>, Kishida and colleagues observed a strong correlation between MH’s dopamine levels and the market value during the investment task.<span class="citation">(<a href="references.html#ref-kishida2011" role="doc-biblioref">Kishida et al. 2011</a>)</span> In 2016, Kishida and colleagues published results from seventeen humans. Here, they explicitly tested the hypothesis that fluctuations in dopamine released in the human striatum encode TD-RPEs.<span class="citation">(<a href="references.html#ref-kishida2016" role="doc-biblioref">Kishida et al. 2016</a>)</span> For the sequential investment task, RPEs were calculated as the difference between an investment’s return on a given trial relative to the expectation defined by the average return from preceding trials.</p>
<p>Contrary to the hypothesis that dopamine fluctuations in the striatum should track TD-RPEs,<span class="citation">(<a href="references.html#ref-montague2004" role="doc-biblioref">P. R. Montague, Hyman, and Cohen 2004</a>; <a href="references.html#ref-montague1996" role="doc-biblioref">P. Montague, Dayan, and Sejnowski 1996</a>; <a href="references.html#ref-schultz1997a" role="doc-biblioref">W. Schultz, Dayan, and Montague 1997b</a>; <a href="references.html#ref-bayer2005a" role="doc-biblioref">Bayer and Glimcher 2005b</a>; <a href="references.html#ref-hart2014" role="doc-biblioref">Hart et al. 2014</a>; <a href="references.html#ref-roesch2007" role="doc-biblioref">Roesch, Calu, and Schoenbaum 2007</a>)</span> the authors found that dopamine fluctuations encode an integration of RPEs with counterfactual prediction errors (CPEs). As discussed in the last chapter, counterfactual signals are an explicit comparison between the present state to alternative ones.<span class="citation">(<a href="references.html#ref-nealj.roese1997" role="doc-biblioref">Neal J. Roese 1997</a>; <a href="references.html#ref-liu2016a" role="doc-biblioref">Liu et al. 2016</a>)</span> For the sequential investment task, CPEs are defined as the difference between the participant’s actual return for a trial and what could have been had they invested more or less. This means that dopamine release is a result of the integration of RPEs and CPEs for a given trial. Formally:</p>
<span class="math display" id="eq:rpe-cpe">\[\begin{equation}
\begin{aligned}
\text{dopamine transient} &amp;\propto \text{RPE} - \text{CPE} \\
&amp;\propto \text{RPE} - r_{tr}(1 - b_{tr})
\end{aligned}
\tag{3.4}
\end{equation}\]</span>
<p>where <span class="math inline">\(b_{tr}\)</span> is the individual’s factional investment at trial <span class="math inline">\(tr\)</span> and <span class="math inline">\(r_{tr}\)</span> is the relative change in market price. As Kishida and colleagues note, the intuition for Equation <a href="neurobiology-of-decision-making.html#eq:rpe-cpe">(3.4)</a> is that better-than-expected outcomes (positive RPEs, increased dopamine release), that <em>could have been even better</em> (positive CPEs) should be reduced in value. Similarly, worse-than-expected outcomes (negative RPEs, decreased dopamine release), that <em>could have been even worse</em> (negative CPEs) should be increased in value. Importantly, these empirical terms are consistent with the subjective feelings (e.g., regret and relief), one experiences in light of a given outcome.</p>
<p>Figure <a href="neurobiology-of-decision-making.html#fig:kishida2016-rpe-inversion">3.2</a> depicts changes in dopamine levels as a function of bet size. Consider the ‘higher bets’ panel (left) where individuals bet close to the maximum amount possible. When they were rewarded with a positive change in portfolio value, the dopamine concentrations increased (green; positive RPEs). Similarly, when the portfolio value decreased, dopamine levels dipped (red; negative RPEs). With high bets, the difference between what they earned and what they could have earned had they bet more is minimal, as is the CPE. With medium bets (middle panel), however, the difference between what individuals experienced and what they could have experienced had they bet more increases. This means that the absolute magnitude of the CPE goes up which subtracts from the dopamine release predicted by positive and negative RPEs. When the CPEs are maximal (small bets; right panel), we observe an inversion in dopamine release in response to positive and negative RPEs.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kishida2016-rpe-inversion"></span>
<img src="figures/kishida2016-rpe-inversion.png" alt="RPE encoding by dopamine transients invert as a function of bet size. Dopamine responses to equal absolute magnitude positive and negative RPEs (−0.75 &gt; RPE &gt; +0.75) when bets are high (higher bets, 100–90%) (Left), medium (medium bets, 80–60%) (Center), or low (lower bets, 50–10%) (Right). For all three plots, mean normalized dopamine responses (±SEM) to positive RPEs (green traces) and negative RPEs (red traces). Inset legends show sample sizes for event types. Two-way ANOVA (RPE-sign and time: 700 ms following and including outcome reveal) reveals a significant difference comparing dopamine responses for positive and negative RPEs following higher bets [FRPE-sign(1,7) = 21.17, P = 0.00] and lower bets [FRPE-sign(1,7) = 32.64, P = 0.00] but not medium bet sizes [FRPE-sign(1,7) = 0.15, P = 0.6957]. Asterisks indicate significant difference between red and green traces: P &lt; 0.05, post hoc, two-sample t test following ANOVA with time and RPE-sign as the two main factors. Asterisks with parentheses indicate Bonferroni correction for multiple comparisons. For low bets (i.e., large CPEs), only those events where the market price change and the RPE-sign are the same are considered. Horizontal axis: time (ms) from outcome reveal (blue arrowhead); vertical axis: mean change in normalized dopamine response.*Reprinted with permission from Kishida et al., 2016.*" width="90%"><p class="caption">
Figure 3.2: RPE encoding by dopamine transients invert as a function of bet size. Dopamine responses to equal absolute magnitude positive and negative RPEs (−0.75 &gt; RPE &gt; +0.75) when bets are high (higher bets, 100–90%) (Left), medium (medium bets, 80–60%) (Center), or low (lower bets, 50–10%) (Right). For all three plots, mean normalized dopamine responses (±SEM) to positive RPEs (green traces) and negative RPEs (red traces). Inset legends show sample sizes for event types. Two-way ANOVA (RPE-sign and time: 700 ms following and including outcome reveal) reveals a significant difference comparing dopamine responses for positive and negative RPEs following higher bets [FRPE-sign(1,7) = 21.17, P = 0.00] and lower bets [FRPE-sign(1,7) = 32.64, P = 0.00] but not medium bet sizes [FRPE-sign(1,7) = 0.15, P = 0.6957]. Asterisks indicate significant difference between red and green traces: P &lt; 0.05, post hoc, two-sample t test following ANOVA with time and RPE-sign as the two main factors. Asterisks with parentheses indicate Bonferroni correction for multiple comparisons. For low bets (i.e., large CPEs), only those events where the market price change and the RPE-sign are the same are considered. Horizontal axis: time (ms) from outcome reveal (blue arrowhead); vertical axis: mean change in normalized dopamine response.<em>Reprinted with permission from Kishida et al., 2016.</em>
</p>
</div>
<p>The ‘superposed error signals about actual and counterfactual reward’ described in Kishida and colleagues’ paper<span class="citation">(<a href="references.html#ref-kishida2016" role="doc-biblioref">Kishida et al. 2016</a>)</span> directly inspired my thesis work. They provide an empirical, neurobiologically plausible framework for investigating decision-making under risk. The theoretical and quantitative underpinnings of classical and behavioral economic theories of decision-making align well with the computational reinforcement learning literature described in this chapter. The explicit probability and outcome structure for risky prospects afford the opportunity to internalize future states and (potentially) incorporate anticipated counterfactual events into the decision-making process. In the next chapter, I derive ‘Counterfactual Predicted Utility Theory’ as a new theory of decision-making under risk.</p>

</div>

  <div class="chapter-nav">
<div class="prev"><a href="evolution-of-decision-theory.html"><span class="header-section-number">2</span> Evolution of Decision Theory</a></div>
<div class="next"><a href="counterfactual-predicted-utility-theory.html"><span class="header-section-number">4</span> Counterfactual Predicted Utility Theory</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav"><li><a class="nav-link" href="#neurobiology-of-decision-making"><span class="header-section-number">3</span> Neurobiology of Decision-Making</a></li></ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/jdtrat/masters-thesis/blob/main/thesis-documents/02_neurobiology-of-decision-making.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/jdtrat/masters-thesis/edit/main/thesis-documents/02_neurobiology-of-decision-making.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Counterfactuals, Dopamine, and Risky Behavior</strong>: Deriving a neurobiological theory of decision-making under risk" was written by Jonathan D. Trattner. It was last built on 2022-04-01.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
