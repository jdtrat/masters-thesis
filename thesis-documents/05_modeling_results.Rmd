---
bibliography: references.bib
---

# Modeling Methods and Results

Using the computational modeling techniques I introduced last chapter, I examined the ability of counterfactual predicted utility theory to explain human choice behavior on the sure bet or gamble task[@liebenow2021]. To do this, I first simulated choice behavior generated from counterfactual predicted utility theory to confirm that the experimental design elicits behaviors assumed in the model.[@wilson2019] With this confirmation, I fit variations of counterfactual predicted utility theory with and without risk-aversion parameter of expected utility theory and counterfactual predicted utility theory with that the task was able to relative to the veracity of counterfactual predicted utility theory as a generative theory of decision-making under risk. I did this by comparing the mode

First up is the explicit simulation and parameter recovery. In order to determine whether or not the sure bet or gamble task could capture the parameters of interest, I simulated fifty subjects

## Simulating Choice Data

As a first step to simulate choice data on the sure-bet or gamble task, I looked at the results of Kahneman and Tversky's prospect theory[@kahneman1979] paper in search of information to inform a prior distribution. Although there is no record for each subject's response on specific prospects, Kahneman and Tversky report the proportion of people that chose each option. For each of the nine prospects that follow the form depicted in Figures \@ref(fig:prospect-diagram-general) and \@ref(fig:prospect-diagram-kt), I used the reported proportions to simulate choice behavior of one-hundred subjects.

I assumed that $\gamma$ was uniformly distributed between zero and one and fixed the softmax sensitivity parameter, $\tau = 1$. I ran ten-thousand iterations using the Metropolis-Hastings algorithm described in the previous chapter. The posterior distribution, depicted in Figure \@ref(fig:kt-post-approx), approximates a $\text{Beta}(1.1,1.1)$ distribution. This means that, in estimating $\gamma$ with the choice proportion data from Kahneman and Tversky, it's plausible that $\gamma$ is any value between zero and one, though a little less likely towards the tails.

```{r kt-post-approx}
#| echo = FALSE, warning = FALSE, fig.align = "center", fig.width = 1.619 * 5, fig.height = 5, dpi = 300, out.width = "95%",
#| fig.cap = "Posterior distribution of $\\gamma$ as estimated from ten-thousand iterations of a Metropolis-Hastings algorithm assuming a uniform prior with choice proportion data from Kahneman and Tversyk's prospect theory[@kahneman1979]. This posterior distribution (orange-red line) is approximated with a $\\text{Beta}(1.1,1.1)$ distribution (blue-gray), indicating slightly less plausibility to $\\gamma$ values near the bounds of the zero-to-one interval relative to central values."
knitr::include_graphics(
  here::here(
    targets::tar_read(kt_post_approx_figure_file)
  )
)
```

Given the task design, participants saw a random subset of 252 prospects. The posterior distribution for $\gamma$ recovered from Kahneman and Tversky's choice proportion data was used to generate $\gamma$ values for fifty subjects. I then simulated their choices on each of the 252 prospects, again fixing the softmax sensitivity parameter $\tau = 1$. For five-thousand iterations of the Metropolis-Hastings algorithm, I sampled from the posterior distribution for each simulated subject.[^1] With the same simulated choice data, I also fit a hierarchical Bayesian model in Stan with the non-centered reparameterization as described in the last chapter. I ran the Hamiltonian Monte Carlo (Stan) sampler for 5000 iterations across four parallel sampling chains for a total of 20,000 samples from the posterior.[^2] Figure \@ref(fig:sbg-sim-parameter-recovery) shows the 95% highest density interval recovered from each sampler's posterior distribution.

[^1]: Although I simulated participants' counterfactual weighting term following the posterior from Kahneman and Tversky's choice proportion data, $\gamma \sim \text{Beta}(1.1,1.1)$, I used a uniform prior again when sampling from the Metropolis-Hastings algorithm for optimizing my analytical workflow. I felt this was justified because $\text{Beta}(1.1,1.1)$ is (relatively) uninformative prior and the decision would not have a large impact on the recoverability of parameter values (with the possible exception of model fitting time). Further, I wanted to maintain consistency with the uniform distribution of the individual-level priors defined in the hierarchical Bayesian model with the inverse Probit transformation.[@ahn2014; @ahn2017]

[^2]: I included a warmup of 2000 iterations, during which time the Hamiltonian Monte Carlo sampling algorithm was tuned to improve efficiency when sampling from the posterior distribution. See [Chapter 9 of the CmdStan User's Guide](https://mc-stan.org/docs/2_29/cmdstan-guide/mcmc-config.html) for more information on 'MCMC Sampling using Hamiltonian Monte Carlo'[@standevelopmentteam2022]

```{r sbg-sim-parameter-recovery}
#| echo = FALSE, warning = FALSE, fig.align = "center", fig.width = 1.619 * 5, fig.height = 8, dpi = 300, out.width = "95%",
#| fig.cap = "95% highest density interval of the posterior distribution of $\\gamma$ as estimated from five-thousand iterations of a Metropolis-Hastings algorithm and five-thousand iterations across four parallel chains with the Hamiltonian Monte Carlo Sampler (estimated using Stan[@standevelopmentteam2022]). The simulated gamma values for each subject are represented as sky-blue dots if they fall within the 95% highest density interval and red dots if they fall outside of it."
knitr::include_graphics(
  here::here(
    targets::tar_read(sbg_sim_parameter_recovery_figure_file)
  )
)
```

For most subjects, the simulated $\gamma$ values are within the highest-density interval. This suggests that the sure-bet or gamble task is able to elicit the behaviors of interest in a way measurable with counterfactual predicted utility theory.

## Parameter Estimation

With the confirmation that I am able to accurately recover simulated parameters from the sure-bet or gamble task, I move on to the next stage of computational modeling, parameter estimation. Specifically, I compare how counterfactual predicted utility theory explains the human choice data from the sure-bet or gamble task relative to expected utility theory. I primarily do this for two reasons:

1.  Counterfactual predicted utility theory suggests that, if $\gamma = 0$, the counterfactual utility of an option is equivalent to the expected value. The expected value is a special case of expected utility theory where the risk aversion parameter, $\rho = 1$.
2.  The sure-bet or gamble task does not include losses, which prohibits me from comparing counterfactual predicted utility theory to prospect theory.

In total, I estimated parameters for seven variants of expected utility theory and counterfactual utility theory on the human choice data (Table \@ref(fig:model-definition-table)).

```{r model-definition-table}
#| echo = FALSE, warning = FALSE, fig.align = "center", dpi = 300, out.width = "95%",
#| fig.cap = "Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task. All models were sampled with Stan for 5000 iterations across four parallel chains using a hierarchical implementation.[@ahn2017] Individual parameters were drawn from a normally distributed group-level distribution and bounded with an inverse Probit transformation resulting in a uniform prior spanning from zero to one $(\\gamma)$, two $(\\rho)$, and thirty $(\\tau)$."

knitr::include_graphics(
  here::here(
    targets::tar_read(model_definition_table_file)
  )
)
```

All models were sampled for 5000 iterations across four parallel chains with the hierarchical Bayesian model formulation previously described.[@ahn2017] For each model, chain convergence for group-level and transformed individual-level parameters was checked with Gelman-Rubin statistics, $\hat{R} \leq 1.1$, suggesting between-chain variance is lower than within chain variance.[@gelman1992] A histogram representing samples from the population level posterior distribution for each parameter per model is shown in Figure \@ref(fig:population-level-post-param-figure).

```{r population-level-post-param-figure}
#| echo = FALSE, warning = FALSE, fig.align = "center",
#| fig.cap = "Placeholder caption."

cowplot::ggdraw() +
  cowplot::draw_label("Placeholder for Population Level Posterior Parameter Estimates") +
  cowplot::draw_line(0:1, 0.1) +
  cowplot::draw_line(0:1, 0.9)

```

## Model Comparison

With the estimated posterior distributions for each model type, I sought to determine which model best explains choice behavior. To do this, I used three different methods to compare models, the results of which are summarized in Table \@ref(fig:model-fit-summary-table):

1.  Posterior predictive checks where, for each model, I simulated choices given the (joint) posterior distribution of each participants' estimated model parameter(s). This was included in Stan's generated quantities block, which is only executed after a posterior sample has been generated. [@standevelopmentteam2022] I then compared the percentage of predicted choices that matches the observed data and summarized the mean and standard deviation for each model.

2.  Comparing the marginal likelihoods of each model. This is the probability of observing the choice behavior for a given model, $M$, $P(\text{Choices}|M)$. The marginal likelihood of each model was estimated using bridge sampling.[@gronau2020] Marginal likelihoods are often included in calculations of Bayes factors, which describes the relative evidence in favor of one model over another by quantifying the ratio between the probability of observing the data given two models. The marginal likelihoods are computed as log-scaled for computational efficiency, which means that the more positive, or less negative, marginal likelihood indicates a better fit.

3.  Assessing penalized model fit with each model's leave-one-out cross validation predictive accuracy.[@vehtari2017] This is approximated with importance sampling of the posterior distribution to calculate the expected log pointwise predictive density (ELPD), which is the logged sum of pointwise posterior predictive distribution for held out data. By multiplying the ELPD by negative two, we get the leave-one-out information criterion, LOOIC. This transformation makes it easier to compare with other information criterion (e.g., AIC, DIC), highlighting the penalization of model complexity.[@plummer2008] I include both ELPD and LOOIC for easy comparison. Note that a less negative ELPD and a smaller (closer to zero) LOOIC are indicative of better model fits.


```{r model-fit-summary-table}
#| echo = FALSE, warning = FALSE, fig.align = "center", dpi = 300, out.width = "95%",
#| fig.cap = "Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task along with model comparison metrics. Posterior predictive choice accuracy represents the mean and standard deviation of correctly predicted choices for individual participants given simulations from the (joint) posterior distribution. ELPD Predictive Density and LOOIC details how well models perform on unobserved data (leave-one-out cross validation). Parantheses for ELPD and LOOIC indicate Monte Carlo sampling error. Marginal likelihood model evidence indicates the plausibility of the data given each model with parantheses representing the interquartile range of the likeihood estimations. In general, better models have smaller LOOIC and higher posterior predictive choice accuracy, ELPD predictive accuracy (less negative), and marginal likelihood model evidence (less negative). CPUT + Softmax Sensitivity and EUT + Softmax Sensitivity are highlighted for easy reference when discussed."

knitr::include_graphics(
  here::here(
    targets::tar_read(model_fit_summary_table_file)
  )
)
```
