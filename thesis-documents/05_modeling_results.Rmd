---
bibliography: references.bib
---

# Modeling Methods and Results

Using the computational modeling techniques I introduced in the previous chapter, I examined the ability of counterfactual predicted utility theory to explain human choice behavior on the sure bet or gamble task[@liebenow2021]. In this chapter, I describe how I implemented the three stages from Wilson and and Collins previously described.[@wilson2019] The first step: simulating choice behavior generated from counterfactual predicted utility theory to confirm that the experimental design elicits behaviors assumed in the model.

## Simulating Choice Data

As a first step to simulate choice data on the sure-bet or gamble task, I looked at the results of Kahneman and Tversky's prospect theory[@kahneman1979] paper in search of information to inform a prior distribution. Although there is no record for each subject's response on specific prospects, Kahneman and Tversky report the proportion of people that chose each option. For each of the nine prospects that follow the form depicted in Figures \@ref(fig:prospect-diagram-general) and \@ref(fig:prospect-diagram-kt), I used the reported proportions to simulate choice behavior of one-hundred subjects.

I assumed that $\gamma$ was uniformly distributed between zero and one and fixed the softmax sensitivity parameter, $\tau = 1$. I ran ten-thousand iterations using the Metropolis-Hastings algorithm described in the previous chapter. The posterior distribution, depicted in Figure \@ref(fig:kt-post-approx), approximates a $\text{Beta}(1.1,1.1)$ distribution. This means that, in estimating $\gamma$ with the choice proportion data from Kahneman and Tversky, it's plausible that $\gamma$ is any value between zero and one, though a little less likely towards the tails.

```{r kt-post-approx}
#| echo = FALSE, warning = FALSE, fig.align = "center", fig.width = 1.619 * 5, fig.height = 5, dpi = 300, out.width = "95%",
#| fig.scap = "Posterior distribution of $\\gamma$ as estimated from ten-thousand iterations of a Metropolis-Hastings algorithm assuming a uniform prior with choice proportion data from Kahneman and Tversyk's prospect theory,",
#| fig.cap = "Posterior distribution of $\\gamma$ as estimated from ten-thousand iterations of a Metropolis-Hastings algorithm assuming a uniform prior with choice proportion data from Kahneman and Tversyk's prospect theory[@kahneman1979]. This posterior distribution (orange-red line) is approximated with a $\\text{Beta}(1.1,1.1)$ distribution (blue-gray), indicating slightly less plausibility to $\\gamma$ values near the bounds of the zero-to-one interval relative to central values."
knitr::include_graphics(
  here::here(
    targets::tar_read(kt_post_approx_figure_file)
  )
)
```

Given the task design, participants saw a random subset of 252 prospects. The posterior distribution for $\gamma$ recovered from Kahneman and Tversky's choice proportion data was used to generate $\gamma$ values for fifty subjects. I then simulated their choices on each of the 252 prospects, again fixing the softmax sensitivity parameter $\tau = 1$. For five-thousand iterations of the Metropolis-Hastings algorithm, I sampled from the posterior distribution for each simulated subject.[^model-results-1] With the same simulated choice data, I also fit a hierarchical Bayesian model in Stan with the non-centered reparameterization as described in the computational modeling concepts chapter. I ran the Hamiltonian Monte Carlo (Stan) sampler for 5000 iterations across four parallel sampling chains for a total of 20,000 samples from the posterior.[^model-results-2] Figure \@ref(fig:sbg-sim-parameter-recovery) shows the 95% highest density interval recovered from each sampler's posterior distribution.

[^model-results-1]: Although I simulated participants' counterfactual weighting term following the posterior from Kahneman and Tversky's choice proportion data, $\gamma \sim \text{Beta}(1.1,1.1)$, I used a uniform prior again when sampling from the Metropolis-Hastings algorithm for optimizing my analytical workflow. I felt this was justified because $\text{Beta}(1.1,1.1)$ is (relatively) uninformative prior and the decision would not have a large impact on the recoverability of parameter values (with the possible exception of model fitting time). Further, I wanted to maintain consistency with the uniform distribution of the individual-level priors defined in the hierarchical Bayesian model with the inverse Probit transformation.[@ahn2014; @ahn2017]

[^model-results-2]: I included a warmup of 2000 iterations, during which time the Hamiltonian Monte Carlo sampling algorithm was tuned to improve efficiency when sampling from the posterior distribution. See [Chapter 9 of the CmdStan User's Guide](https://mc-stan.org/docs/2_29/cmdstan-guide/mcmc-config.html) for more information on 'MCMC Sampling using Hamiltonian Monte Carlo'[@standevelopmentteam2022]

```{r sbg-sim-parameter-recovery}
#| echo = FALSE, warning = FALSE, fig.align = "center", fig.width = 1.619 * 5, fig.height = 8, dpi = 300, out.width = "95%",
#| fig.scap = "95 percent highest density interval of the posterior distribution of $\\gamma$ for fifty simulated subjects.",
#| fig.cap = "95 percent highest density interval of the posterior distribution of $\\gamma$ as estimated from five-thousand iterations of a Metropolis-Hastings algorithm and five-thousand iterations across four parallel chains with the Hamiltonian Monte Carlo Sampler (estimated using Stan). The simulated gamma values for each subject are represented as sky-blue dots if they fall within the highest density interval and red dots if they fall outside of it."
knitr::include_graphics(
  here::here(
    targets::tar_read(sbg_sim_parameter_recovery_figure_file)
  )
)
```

For most subjects, the simulated $\gamma$ values are within the highest-density interval. This suggests that the sure-bet or gamble task is able to elicit the behaviors of interest in a way measurable with counterfactual predicted utility theory.

## Parameter Estimation

With the confirmation that I am able to accurately recover simulated parameters from the sure-bet or gamble task, I move on to the next stage of computational modeling, parameter estimation. Specifically, I compare how counterfactual predicted utility theory explains the human choice data from the sure-bet or gamble task relative to expected utility theory. I primarily do this for two reasons:

1.  Counterfactual predicted utility theory suggests that, if $\gamma = 0$, the counterfactual utility of an option is equivalent to the expected value. The expected value is a special case of expected utility theory where the risk aversion parameter, $\rho = 1$.
2.  The sure-bet or gamble task does not include losses, which prohibits me from comparing counterfactual predicted utility theory to prospect theory.

In total, I estimated parameters for seven variants of expected utility theory and counterfactual utility theory on the human choice data (Table \@ref(fig:model-definition-table)).

```{r model-definition-table}
#| echo = FALSE, warning = FALSE, fig.align = "center", dpi = 300, out.width = "95%",
#| fig.scap = "Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task.",
#| fig.cap = "Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task. All models were sampled with Stan for 5000 iterations across four parallel chains using a hierarchical implementation. Individual parameters were drawn from a normally distributed group-level distribution and bounded with an inverse Probit transformation resulting in a uniform prior spanning from zero to one $(\\gamma)$, two $(\\rho)$, and thirty $(\\tau)$."

knitr::include_graphics(
  here::here(
    targets::tar_read(model_definition_table_file)
  )
)
```

All models were sampled for 5000 iterations across four parallel chains with the hierarchical Bayesian model formulation previously described.[@ahn2017] For each model, chain convergence for group-level and transformed individual-level parameters was checked with Gelman-Rubin statistics, $\hat{R} \leq 1.1$, suggesting between-chain variance is lower than within chain variance.[@gelman1992] The group-level posterior distributions for parameters fit from each model are shown in Figure \@ref(fig:population-level-post-param-figure).

```{r population-level-post-param-figure}
#| echo = FALSE, warning = FALSE, fig.align = "center", dpi = 300, out.width = "95%",
#| fig.scap = "Posterior distribution of group-level parameter estimates for each model.",
#| fig.cap = "Posterior distribution of group-level parameter estimates for each model. Distributions for $\\gamma$ are in green, $\\rho$ in blue, and $\\tau$ in orange."

knitr::include_graphics(
  here::here(
    targets::tar_read(population_posterior_plot_file)
  )
)

```

## Model Comparison

With the estimated posterior distributions for each model type, I sought to determine which model best explains choice behavior. To do this, I used three different methods to compare models, the results of which are summarized in Table \@ref(fig:model-fit-summary-table):

1.  Posterior predictive checks where, for each model, I simulated choices given the (joint) posterior distribution of each participants' estimated model parameter(s). This was included in Stan's generated quantities block, which is only executed after a posterior sample has been generated. [@standevelopmentteam2022] I then compared the percentage of predicted choices that matches the observed data and summarized the mean and standard deviation for each model.

2.  Comparing the marginal likelihoods of each model. This is the probability of observing the choice behavior for a given model, $M$, $P(\text{Choices}|M)$. The marginal likelihood of each model was estimated using bridge sampling.[@gronau2020] Marginal likelihoods are often included in calculations of Bayes factors, which describes the relative evidence in favor of one model over another by quantifying the ratio between the probability of observing the data given two models. The marginal likelihoods are computed as log-scaled for computational efficiency, which means that the more positive, or less negative, marginal likelihood indicates a better fit.

3.  Assessing penalized model fit with each model's leave-one-out cross validation predictive accuracy.[@vehtari2017] This is approximated with importance sampling of the posterior distribution to calculate the expected log pointwise predictive density (ELPD), which is the logged sum of pointwise posterior predictive distribution for held out data. By multiplying the ELPD by negative two, we get the leave-one-out information criterion, LOOIC. This transformation makes it easier to compare with other information criterion (e.g., AIC, DIC), highlighting the penalization of model complexity.[@plummer2008] I include both ELPD and LOOIC for easy comparison. Note that a less negative ELPD and a smaller (closer to zero) LOOIC are indicative of better model fits.


```{r model-fit-summary-table}
#| echo = FALSE, warning = FALSE, fig.align = "center", dpi = 300, out.width = "95%",
#| fig.scap = "Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task along with model comparison metrics.",
#| fig.cap = "Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task along with model comparison metrics. Posterior predictive choice accuracy represents the mean and standard deviation of correctly predicted choices for individual participants given simulations from the (joint) posterior distribution. ELPD Predictive Density and LOOIC details how well models perform on unobserved data (leave-one-out cross validation). Parantheses for ELPD and LOOIC indicate Monte Carlo sampling error. Marginal likelihood model evidence indicates the plausibility of the data given each model with parantheses representing the interquartile range of the likeihood estimations. In general, better models have smaller LOOIC and higher posterior predictive choice accuracy, ELPD predictive accuracy (less negative), and marginal likelihood model evidence (less negative). CPUT + Softmax Sensitivity and EUT + Softmax Sensitivity are highlighted for easy reference when discussed."

knitr::include_graphics(
  here::here(
    targets::tar_read(model_fit_summary_table_file)
  )
)
```

The two rows highlighted in Table \@ref(fig:model-fit-summary-table), represent the most direct comparison of contractual predicted utility theory with expected utility theory. The metrics presented pose an interesting problem with the blanket statement that a 'better model' has a  smaller LOOIC and higher (or less negative) posterior predictive choice accuracy, ELPD, and marginal likelihood model evidence, the model fit metrics pose an interesting problem. 

On average, both — in fact, all models — accurately predict more than 80% of observed choices using the posterior predictive distribution. If we look at either ELPD or LOOIC, we see that the expected utility model is performs better. Looking at the marginal likelihood, however, the counterfactual predicted utility model seems better. These seemingly disparate indicators make sense when considering what each metric represents. In the next chapter, I will conclude my thesis with a discussion of these results and outline to next steps to contribute towards a better understanding of the neurobiological basis of decision-making under risk.
