<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>8 Computational Modeling Methods and Results | Counterfactuals, Dopamine, and Risky Behavior</title>
<meta name="author" content="Jonathan D. Trattner">
<meta name="description" content="Using the computational modeling techniques I introduced in the previous chapter, I examined the ability of counterfactual predicted utility theory to explain human choice behavior on the sure bet...">
<meta name="generator" content="bookdown 0.25 with bs4_book()">
<meta property="og:title" content="8 Computational Modeling Methods and Results | Counterfactuals, Dopamine, and Risky Behavior">
<meta property="og:type" content="book">
<meta property="og:url" content="https://masters-thesis.jdtrat.com/computational-modeling-methods-and-results.html">
<meta property="og:description" content="Using the computational modeling techniques I introduced in the previous chapter, I examined the ability of counterfactual predicted utility theory to explain human choice behavior on the sure bet...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="8 Computational Modeling Methods and Results | Counterfactuals, Dopamine, and Risky Behavior">
<meta name="twitter:site" content="@jdtrat">
<meta name="twitter:description" content="Using the computational modeling techniques I introduced in the previous chapter, I examined the ability of counterfactual predicted utility theory to explain human choice behavior on the sure bet...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Deriving a neurobiological theory of decision-making under risk">Counterfactuals, Dopamine, and Risky Behavior</a>:
        <small class="text-muted">Deriving a neurobiological theory of decision-making under risk</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Dedication and Acknowledgements</a></li>
<li><a class="" href="abstract.html"><span class="header-section-number">2</span> Abstract</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">3</span> Introduction</a></li>
<li><a class="" href="evolution-of-decision-theory.html"><span class="header-section-number">4</span> Evolution of Decision Theory</a></li>
<li><a class="" href="neurobiology-of-decision-making.html"><span class="header-section-number">5</span> Neurobiology of Decision-Making</a></li>
<li><a class="" href="counterfactual-predicted-utility-theory.html"><span class="header-section-number">6</span> Counterfactual Predicted Utility Theory</a></li>
<li><a class="" href="computational-modeling-concepts.html"><span class="header-section-number">7</span> Computational Modeling Concepts</a></li>
<li><a class="active" href="computational-modeling-methods-and-results.html"><span class="header-section-number">8</span> Computational Modeling Methods and Results</a></li>
<li><a class="" href="discussion-next-steps.html"><span class="header-section-number">9</span> Discussion &amp; Next Steps</a></li>
<li><a class="" href="references.html"><span class="header-section-number">10</span> References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/jdtrat/masters-thesis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="computational-modeling-methods-and-results" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Computational Modeling Methods and Results<a class="anchor" aria-label="anchor" href="#computational-modeling-methods-and-results"><i class="fas fa-link"></i></a>
</h1>
<p>Using the computational modeling techniques I introduced in the previous chapter, I examined the ability of counterfactual predicted utility theory to explain human choice behavior on the sure bet or gamble task<span class="citation"><sup>[<a href="references.html#ref-liebenow2021" role="doc-biblioref">35</a>]</sup></span>. In this chapter, I present my methods and results concurrently following the three stages adapted from Wilson and Collins discussed in Section <a href="computational-modeling-concepts.html#computational-modeling-concepts">7</a>.<span class="citation"><sup>[<a href="references.html#ref-wilson2019" role="doc-biblioref">36</a>]</sup></span> I start by simulating choice behavior generated from counterfactual predicted utility theory to confirm that the experimental design elicits behaviors assumed in the model. I then find estimate individual- and group-level parameters for three candidate models. Lastly, I quantify model fit with model comparison techniques.</p>
<div id="simulating-choice-data" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Simulating Choice Data<a class="anchor" aria-label="anchor" href="#simulating-choice-data"><i class="fas fa-link"></i></a>
</h2>
<p>As a first step to simulate choice data on the sure-bet or gamble task, I looked at the results of Kahneman and Tversky’s prospect theory<span class="citation"><sup>[<a href="references.html#ref-kahneman1979" role="doc-biblioref">4</a>]</sup></span> paper in search of information to inform a prior distribution. Although there is no record for each subject’s response on specific prospects, Kahneman and Tversky report the proportion of people that chose each option. For each of the nine prospects that follow the form depicted in Figures <a href="counterfactual-predicted-utility-theory.html#fig:prospect-diagram-general">6.1</a> and <a href="counterfactual-predicted-utility-theory.html#fig:prospect-diagram-kt">6.2</a>, I used the reported proportions to simulate choice behavior of one-hundred subjects.</p>
<p>I assumed that <span class="math inline">\(\gamma\)</span> was uniformly distributed between zero and one and fixed the softmax sensitivity parameter, <span class="math inline">\(\tau = 2.2\)</span> following the conceptual visualizations from Section <a href="computational-modeling-concepts.html#visualizing-bayes-theorem-with-choice-datamodeling-concepts-2">7.2</a> consistent with the literature.<span class="citation"><sup>[<a href="references.html#ref-sokol-hessner2009" role="doc-biblioref">39</a>]</sup></span> I ran ten-thousand iterations using the Metropolis-Hastings algorithm described in the previous chapter. The posterior distribution, depicted in Figure <a href="computational-modeling-methods-and-results.html#fig:kt-post-approx">8.1</a>, approximates a <span class="math inline">\(\text{Beta}(1.1,1.1)\)</span> distribution. This means that, in estimating <span class="math inline">\(\gamma\)</span> with the choice proportion data from Kahneman and Tversky, it’s plausible that <span class="math inline">\(\gamma\)</span> is any value between zero and one, though a little less likely towards the tails.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kt-post-approx"></span>
<img src="figures/kt_post_approx_figure.png" alt="Posterior distribution of $\gamma$ as estimated from ten-thousand iterations of a Metropolis-Hastings algorithm assuming a uniform prior with choice proportion data from Kahneman and Tversky's prospect theory. This posterior distribution (orange-red line) is approximated with a $\text{Beta}(1.1,1.1)$ distribution (blue-gray), indicating slightly less plausibility to $\gamma$ values near the bounds of the zero-to-one interval relative to central values." width="95%"><p class="caption">
Figure 8.1: Posterior distribution of <span class="math inline">\(\gamma\)</span> as estimated from ten-thousand iterations of a Metropolis-Hastings algorithm assuming a uniform prior with choice proportion data from Kahneman and Tversky’s prospect theory. This posterior distribution (orange-red line) is approximated with a <span class="math inline">\(\text{Beta}(1.1,1.1)\)</span> distribution (blue-gray), indicating slightly less plausibility to <span class="math inline">\(\gamma\)</span> values near the bounds of the zero-to-one interval relative to central values.
</p>
</div>
<p>Given the sure bet or gamble task design, participants saw a random subset of 252 prospects. The posterior distribution for <span class="math inline">\(\gamma\)</span> recovered from Kahneman and Tversky’s choice proportion data was used to generate <span class="math inline">\(\gamma\)</span> values for fifty subjects. I then simulated their choices on each of the 252 prospects, again fixing the softmax sensitivity parameter <span class="math inline">\(\tau = 2.2\)</span>. For five-thousand iterations of the Metropolis-Hastings algorithm, I sampled from the posterior distribution for each simulated subject.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Although I simulated participants’ counterfactual weighting term following the posterior from Kahneman and Tversky’s choice proportion data, &lt;span class="math inline"&gt;\(\gamma \sim \text{Beta}(1.1,1.1)\)&lt;/span&gt;, I used a uniform prior again when sampling from the Metropolis-Hastings algorithm for optimizing my analytical workflow. I felt this was justified because &lt;span class="math inline"&gt;\(\text{Beta}(1.1,1.1)\)&lt;/span&gt; is (relatively) uninformative prior and the decision would not have a large impact on the recoverability of parameter values (with the possible exception of model fitting time). Further, I wanted to maintain consistency with the uniform distribution of the individual-level priors defined in the hierarchical Bayesian model with the inverse Probit transformation.&lt;span class="citation"&gt;&lt;sup&gt;[&lt;a href="references.html#ref-ahn2017" role="doc-biblioref"&gt;44&lt;/a&gt;,&lt;a href="references.html#ref-ahn2014" role="doc-biblioref"&gt;45&lt;/a&gt;]&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;'><sup>10</sup></a> With the same simulated data, I fit a hierarchical Bayesian model in Stan with the non-centered reparameterization as described in the computational modeling concepts chapter. I ran the Hamiltonian Monte Carlo (Stan) sampler for 5000 iterations across four parallel sampling chains for a total of 20,000 samples from the posterior.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;I included a warmup of 2000 iterations, during which time the Hamiltonian Monte Carlo sampling algorithm was tuned to improve efficiency when sampling from the posterior distribution. See &lt;a href="https://mc-stan.org/docs/2_29/cmdstan-guide/mcmc-config.html"&gt;Chapter 9 of the CmdStan User’s Guide&lt;/a&gt; for more information on ‘MCMC Sampling using Hamiltonian Monte Carlo’&lt;span class="citation"&gt;&lt;sup&gt;[&lt;a href="references.html#ref-standevelopmentteam2022" role="doc-biblioref"&gt;42&lt;/a&gt;]&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;'><sup>11</sup></a></p>
<p>I chose to estimate the posterior distribution with both the Metropolis-Hastings and Hamiltonian Monte Carlo algorithms to highlight the similarity in posterior parameter estimation techniques discussed in Chapter <a href="computational-modeling-concepts.html#computational-modeling-concepts">7</a>. Figure <a href="computational-modeling-methods-and-results.html#fig:sbg-sim-parameter-recovery">8.2</a> shows the 95% highest density interval recovered from each sampler’s posterior distribution. For most subjects, the simulated <span class="math inline">\(\gamma\)</span> values are within the highest-density interval. This suggests that the sure-bet or gamble task is able to elicit the behaviors of interest in a way measurable with counterfactual predicted utility theory.</p>
<p>With the confirmation that I am able to accurately recover simulated parameters from the sure-bet or gamble task, I move on to the next stage of computational modeling, parameter estimation. To do so, I use the hierarchical Bayesian model in Stan for more stable and reliable estimates and computational efficiency.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:sbg-sim-parameter-recovery"></span>
<img src="figures/sbg_sim_parameter_recovery_figure.png" alt="95 percent highest density interval of the posterior distribution of $\gamma$ as estimated from five-thousand iterations of a Metropolis-Hastings algorithm and five-thousand iterations across four parallel chains with the Hamiltonian Monte Carlo Sampler (estimated using Stan). The simulated gamma values for each subject are represented as sky-blue dots if they fall within the highest density interval and red dots if they fall outside of it." width="95%"><p class="caption">
Figure 8.2: 95 percent highest density interval of the posterior distribution of <span class="math inline">\(\gamma\)</span> as estimated from five-thousand iterations of a Metropolis-Hastings algorithm and five-thousand iterations across four parallel chains with the Hamiltonian Monte Carlo Sampler (estimated using Stan). The simulated gamma values for each subject are represented as sky-blue dots if they fall within the highest density interval and red dots if they fall outside of it.
</p>
</div>
</div>
<div id="parameter-estimation" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Parameter Estimation<a class="anchor" aria-label="anchor" href="#parameter-estimation"><i class="fas fa-link"></i></a>
</h2>
<p>To assess counterfactual predicted utility theory’s validity as a theory of decision-making under risk, I estimate parameters for it in comparison with expected utility theory. I primarily do this for two reasons:</p>
<ol style="list-style-type: decimal">
<li>Counterfactual predicted utility theory suggests that, if <span class="math inline">\(\gamma = 0\)</span>, the counterfactual utility of an option is equivalent to the expected value. The expected value is a special case of expected utility theory where the risk sensitivity parameter, <span class="math inline">\(\rho = 1\)</span>.</li>
<li>The sure-bet or gamble task does not include losses, which prohibits me from comparing counterfactual predicted utility theory to prospect theory.</li>
</ol>
<p>In addition to directly comparing CPUT with EUT, I wanted to see how different risk preferences might affect interact with counterfactual information to inform choice behavior. In total, I fit three models:</p>
<ul>
<li>
<strong>CPUT + Softmax Sensitivity</strong>: This model looks at how counterfactual information informs choice behavior. Its estimated parameters are the counterfactual weighting term, <span class="math inline">\(\gamma\)</span>, and the sensitivity to differences in choice utilities, <span class="math inline">\(\tau\)</span>.</li>
<li>
<strong>CPUT + Risk Sensitivity + Softmax Sensitivity</strong>: This model looks at how differences in risk sensitivity may interact with counterfactual information to inform choice behavior. Its estimated parameters are the counterfactual weighting term, <span class="math inline">\(\gamma\)</span>, the risk sensitivity term, <span class="math inline">\(\rho\)</span>, and the sensitivity to differences in choice utilities, <span class="math inline">\(\tau\)</span>.</li>
<li>
<strong>EUT + Softmax Sensitivity</strong>: This model looks at how differences in risk sensitivity informs choice behavior. Its estimated parameters are the risk sensitivity term, <span class="math inline">\(\rho\)</span>, and the sensitivity to differences in choice utilities, <span class="math inline">\(\tau\)</span>.</li>
</ul>
<p>All models were sampled for 5000 iterations across four parallel chains with the hierarchical Bayesian model formulation described in Section <a href="computational-modeling-concepts.html#posterior-estimation-with-markov-chain-monte-carlo">7.3</a>.<span class="citation"><sup>[<a href="references.html#ref-ahn2017" role="doc-biblioref">44</a>]</sup></span> For each model, chain convergence for group-level and transformed individual-level parameters was checked with Gelman-Rubin statistics, <span class="math inline">\(\hat{R} \leq 1.1\)</span>, suggesting between-chain variance is lower than within chain variance.<span class="citation"><sup>[<a href="references.html#ref-gelman1992" role="doc-biblioref">46</a>]</sup></span> The group-level posterior distributions for parameters fit from each model are shown in Figure <a href="computational-modeling-methods-and-results.html#fig:population-level-post-param-figure">8.3</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:population-level-post-param-figure"></span>
<img src="figures/population_posterior_plot.png" alt="Posterior distribution of group-level parameter estimates for each model. Relative distributions for $\gamma$ (left), $\rho$ (middle), and $\tau$ (right) underlined by 95 percent highest density intervals for each model with the median indicated. 'CPUT + Softmax Sensitivity' model shown in blue with estimated parameters for $\gamma, \tau$; 'EUT + Softmax Sensitivity' shown in red with estimated parameters for $\rho, \tau$; 'CPUT + Risk Sensitivity + Softmax Sensitivity' shown in yellow with estimated parameters for $\gamma, \rho, \tau$." width="95%"><p class="caption">
Figure 8.3: Posterior distribution of group-level parameter estimates for each model. Relative distributions for <span class="math inline">\(\gamma\)</span> (left), <span class="math inline">\(\rho\)</span> (middle), and <span class="math inline">\(\tau\)</span> (right) underlined by 95 percent highest density intervals for each model with the median indicated. ‘CPUT + Softmax Sensitivity’ model shown in blue with estimated parameters for <span class="math inline">\(\gamma, \tau\)</span>; ‘EUT + Softmax Sensitivity’ shown in red with estimated parameters for <span class="math inline">\(\rho, \tau\)</span>; ‘CPUT + Risk Sensitivity + Softmax Sensitivity’ shown in yellow with estimated parameters for <span class="math inline">\(\gamma, \rho, \tau\)</span>.
</p>
</div>
</div>
<div id="model-comparison" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Model Comparison<a class="anchor" aria-label="anchor" href="#model-comparison"><i class="fas fa-link"></i></a>
</h2>
<p>With the estimated posterior distributions for each model type, I sought to determine which model best explains choice behavior. To do this, I used three different methods to compare models, the results of which are summarized in Table <a href="computational-modeling-methods-and-results.html#fig:model-fit-summary-table">8.4</a>:</p>
<ol style="list-style-type: decimal">
<li><p>Posterior predictive checks where, for each model, I simulated choices given the (joint) posterior distribution of each participants’ estimated model parameter(s). This was included in Stan’s generated quantities block, which is only executed after a posterior sample has been generated.<span class="citation"><sup>[<a href="references.html#ref-standevelopmentteam2022" role="doc-biblioref">42</a>]</sup></span> I then compared the percentage of predicted choices that matches the observed data and summarized the mean and standard deviation for each model.</p></li>
<li><p>Comparing the marginal likelihoods of each model. This is the probability of observing the choice behavior for a given model, <span class="math inline">\(M\)</span>, <span class="math inline">\(P(\text{Choices}|M)\)</span>. The marginal likelihood of each model was estimated using bridge sampling.<span class="citation"><sup>[<a href="references.html#ref-gronau2020" role="doc-biblioref">47</a>]</sup></span> Marginal likelihoods are often included in calculations of Bayes factors, which describes the relative evidence in favor of one model over another by quantifying the ratio between the probability of observing the data given two models. The marginal likelihoods are computed as log-scaled for computational efficiency, which means that the more positive, or less negative, marginal likelihood indicates a better fit.</p></li>
<li><p>Assessing penalized model fit with each model’s leave-one-out cross validation predictive accuracy.<span class="citation"><sup>[<a href="references.html#ref-vehtari2017" role="doc-biblioref">48</a>]</sup></span> This is approximated with importance sampling of the posterior distribution to calculate the expected log pointwise predictive density (ELPD), which is the logged sum of pointwise posterior predictive distribution for held out data. By multiplying the ELPD by negative two, we get the leave-one-out information criterion, LOOIC. This transformation makes it easier to compare with other information criterion (e.g., AIC, DIC), highlighting the penalization of model complexity.<span class="citation"><sup>[<a href="references.html#ref-plummer2008" role="doc-biblioref">49</a>]</sup></span> I include both ELPD and LOOIC for easy comparison. Note that a less negative ELPD and a smaller (closer to zero) LOOIC are indicative of better model fits.</p></li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:model-fit-summary-table"></span>
<img src="figures/model_fit_summary_table.png" alt="Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task along with model comparison metrics. Posterior predictive choice accuracy represents the mean and standard deviation of correctly predicted choices for individual participants given simulations from the (joint) posterior distribution. ELPD Predictive Density and LOOIC details how well models perform on unobserved data (leave-one-out cross validation). Parantheses for ELPD and LOOIC indicate Monte Carlo sampling error. Marginal likelihood model evidence indicates the plausibility of the data given each model with parantheses representing the interquartile range of the likeihood estimations. In general, better models have smaller LOOIC and higher posterior predictive choice accuracy, ELPD predictive accuracy (less negative), and marginal likelihood model evidence (less negative). CPUT + Softmax Sensitivity and EUT + Softmax Sensitivity are highlighted for easy reference when discussed." width="95%"><p class="caption">
Figure 8.4: Parameters for, and description of, the different models fit on human choice data from the sure-bet or gamble task along with model comparison metrics. Posterior predictive choice accuracy represents the mean and standard deviation of correctly predicted choices for individual participants given simulations from the (joint) posterior distribution. ELPD Predictive Density and LOOIC details how well models perform on unobserved data (leave-one-out cross validation). Parantheses for ELPD and LOOIC indicate Monte Carlo sampling error. Marginal likelihood model evidence indicates the plausibility of the data given each model with parantheses representing the interquartile range of the likeihood estimations. In general, better models have smaller LOOIC and higher posterior predictive choice accuracy, ELPD predictive accuracy (less negative), and marginal likelihood model evidence (less negative). CPUT + Softmax Sensitivity and EUT + Softmax Sensitivity are highlighted for easy reference when discussed.
</p>
</div>
<div id="eut-versus-cput" class="section level3" number="8.3.1">
<h3>
<span class="header-section-number">8.3.1</span> EUT versus CPUT<a class="anchor" aria-label="anchor" href="#eut-versus-cput"><i class="fas fa-link"></i></a>
</h3>
<p>Taken together, the model comparison metrics show that EUT provides a better explanation of both the observed data on the sure bet or gamble task (<span class="math inline">\(BF_{\frac{\rho, \tau}{\gamma, \tau}}\)</span> = 48.07) and generalizes better than CPUT (<span class="math inline">\(\text{LOOIC}_{\rho, \tau} - \text{LOOIC}_{\gamma, \tau}\)</span> = -381.34). The posterior predictive choice accuracy – the percentage of choices simulated with the posterior estimate of each subject’s parameters – are similar, though better for EUT (83.8%) compared to CPUT (82.3%).</p>
<p>Further, the group-level posterior parameter distribution for <span class="math inline">\(\tau\)</span>, the common variable to these two models, is higher for EUT than CPUT (95% HDI for EUT is [1.51, 2.62] with median of 2.05; CPUT = [0.97, 1.31] with median of 1.13). This suggests more random utility maximizing decisions for EUT relative to more random choice behavior for CPUT.</p>
</div>
<div id="cput-risk-sensitivity-versus-eut" class="section level3" number="8.3.2">
<h3>
<span class="header-section-number">8.3.2</span> CPUT + Risk Sensitivity versus EUT<a class="anchor" aria-label="anchor" href="#cput-risk-sensitivity-versus-eut"><i class="fas fa-link"></i></a>
</h3>
<p>Interestingly, the risk sensitivity term, <span class="math inline">\(\rho\)</span>, from EUT with the counterfactual weighting from CPUT offers the best explanation for the observed human choice data (<span class="math inline">\(BF_{\frac{\text{CPUT + Softmax + Risk Sensitivity}}{\rho, \tau}}\)</span> = 109.89). This does come with a cost of decreased generalizability (<span class="math inline">\(\text{LOOIC}_{\rho, \tau} - \text{LOOIC}_\text{CPUT + Softmax + Risk Sensitivity}\)</span> = -158.18) and increased posterior predictive choice accuracy (83.9% relative to EUT’s 83.8%).</p>
<p>Further, the group-level posterior parameter distribution for <span class="math inline">\(\tau\)</span> depict similar sensitivities to utility differences (95% HDI for ‘CPUT + Softmax + Risk Sensitivity’ = [1.45, 2.57] with median of 1.98; ‘EUT + Softmax = [1.51, 2.62] with median of 2.05) and risk sensitivity (95% HDI for ’CPUT + Softmax + Risk Sensitivity’ = [0.68, 0.94] with median of 0.8; ‘EUT + Softmax’ = [0.66, 0.93] with median of 0.79).</p>
</div>
<div id="cput-risk-sensitivity-versus-cput" class="section level3" number="8.3.3">
<h3>
<span class="header-section-number">8.3.3</span> CPUT + Risk Sensitivity versus CPUT<a class="anchor" aria-label="anchor" href="#cput-risk-sensitivity-versus-cput"><i class="fas fa-link"></i></a>
</h3>
<p>Incorporating risk sensitivity to CPUT, mixing the value estimate transformation of EUT (left-hand side of Equation <a href="counterfactual-predicted-utility-theory.html#eq:eut-cput-transformations">(6.4)</a>) with that of CPUT (right-hand side of Equation <a href="counterfactual-predicted-utility-theory.html#eq:eut-cput-transformations">(6.4)</a>) results in a better explanation for the observed data (<span class="math inline">\(BF_{\frac{\rho, \gamma, \tau}{\gamma, \tau}}\)</span> = 157.95), is more generalizable, (<span class="math inline">\(\text{LOOIC}_{\rho, \gamma, \tau} - \text{LOOIC}_{\gamma, \tau}\)</span> = -223.16), and has a higher posterior predictive choice accuracy (83.9%) relative to CPUT’s (82.3%).</p>
<p>Interestingly, the group-level posterior parameter distribution for <span class="math inline">\(\tau\)</span> with the CPUT + Risk Sensitivity more closely reflects that of EUT. At the same time, the group-level posterior parameter distribution for <span class="math inline">\(\gamma\)</span> is much more tightly concentrated towards zero than for CPUT (95% HDI for CPUT + Risk Sensitivity is [&lt;0.001, 0.006] with median of 0.002; CPUT = [&lt;0.001, 0.015] with median of 0.006). This suggests more a lower weighting on counterfactual information while accounting for risk preferences.</p>
<hr>
<p>By most measures, it seems that Expected Utility Theory provides the most generalizable explanation of human choice data on the sure bet or gamble task. Looking at the In the next chapter, I discuss these results and outline next steps to contribute towards a better understanding of the neurobiological basis of decision-making under risk.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="computational-modeling-concepts.html"><span class="header-section-number">7</span> Computational Modeling Concepts</a></div>
<div class="next"><a href="discussion-next-steps.html"><span class="header-section-number">9</span> Discussion &amp; Next Steps</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#computational-modeling-methods-and-results"><span class="header-section-number">8</span> Computational Modeling Methods and Results</a></li>
<li><a class="nav-link" href="#simulating-choice-data"><span class="header-section-number">8.1</span> Simulating Choice Data</a></li>
<li><a class="nav-link" href="#parameter-estimation"><span class="header-section-number">8.2</span> Parameter Estimation</a></li>
<li>
<a class="nav-link" href="#model-comparison"><span class="header-section-number">8.3</span> Model Comparison</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#eut-versus-cput"><span class="header-section-number">8.3.1</span> EUT versus CPUT</a></li>
<li><a class="nav-link" href="#cput-risk-sensitivity-versus-eut"><span class="header-section-number">8.3.2</span> CPUT + Risk Sensitivity versus EUT</a></li>
<li><a class="nav-link" href="#cput-risk-sensitivity-versus-cput"><span class="header-section-number">8.3.3</span> CPUT + Risk Sensitivity versus CPUT</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/jdtrat/masters-thesis/blob/main/thesis-documents/05_modeling_results.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/jdtrat/masters-thesis/edit/main/thesis-documents/05_modeling_results.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Counterfactuals, Dopamine, and Risky Behavior</strong>: Deriving a neurobiological theory of decision-making under risk" was written by Jonathan D. Trattner. It was last built on 2022-05-02.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
